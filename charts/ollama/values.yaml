# Ollama Configuration

# Replica count
replicaCount: 1

# Container image
image:
  repository: localhost:5001/ollama
  pullPolicy: IfNotPresent
  tag: "latest"

# Service configuration
service:
  type: ClusterIP
  port: 11434
  targetPort: 11434

# Resource limits
resources:
  requests:
    memory: "512Mi"
    cpu: "200m"
  limits:
    memory: "2Gi"
    cpu: "1000m"

# Node scheduling - schedule on inference node
nodeSelector:
  role: inference

# Tolerate the inference node taint
tolerations:
  - key: "role"
    operator: "Equal"
    value: "inference"
    effect: "NoSchedule"

# Storage for models
persistence:
  enabled: true
  storageClass: "standard"
  accessMode: ReadWriteOnce
  size: 5Gi
  mountPath: /home/ollama/.ollama

# Security context - run as non-root
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - ALL

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

# Models to preload (will be downloaded on startup)
models:
  - tinyllama:1.1b

# ServiceAccount
serviceAccount:
  create: true
  name: "ollama"

# Liveness and readiness probes
livenessProbe:
  enabled: true
  httpGet:
    path: /
    port: 11434
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  enabled: true
  httpGet:
    path: /
    port: 11434
  initialDelaySeconds: 30
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3

# Environment variables
env: []

# Prometheus metrics (disabled - no Prometheus in demo)
metrics:
  enabled: false

# Network Policy for restricting pod communication
networkPolicy:
  enabled: true
