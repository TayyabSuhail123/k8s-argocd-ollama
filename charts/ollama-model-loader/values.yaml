# Ollama Model Loader Configuration

# Ollama image to use for loading models
image:
  repository: localhost:5001/ollama
  tag: "latest"
  pullPolicy: IfNotPresent

# Models to download
models:
  - tinyllama:1.1b

# PVC configuration - must match the Ollama deployment
persistence:
  # Name of the existing PVC created by Ollama chart
  existingClaim: "ollama-data"
  mountPath: /home/ollama/.ollama

# Job configuration
job:
  # Number of retries if job fails
  backoffLimit: 3
  # Time to keep completed job pods (for debugging)
  ttlSecondsAfterFinished: 86400  # 24 hours
  # Pod restart policy
  restartPolicy: OnFailure

# Resource limits for the job
resources:
  requests:
    memory: "256Mi"
    cpu: "200m"
  limits:
    memory: "1Gi"
    cpu: "1000m"

# Security context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - ALL

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault

# Node scheduling - run on inference node where Ollama runs
nodeSelector:
  role: inference

# Tolerations
tolerations:
  - key: "role"
    operator: "Equal"
    value: "inference"
    effect: "NoSchedule"
